{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "810d9f10-ec19-4b09-8f90-e983e460b319",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://laelgelcpublic.s3.sa-east-1.amazonaws.com/lael_50_years_narrow_white.png.no_years.400px_96dpi.png\" width=\"300\" alt=\"LAEL 50 years logo\">\n",
    "<h3>APPLIED LINGUISTICS GRADUATE PROGRAMME (LAEL)</h3>\n",
    "</center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c2c96-2fc3-4a1a-995b-c388036a2a15",
   "metadata": {},
   "source": [
    "# Topical Modelling Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9c2af7-9fc1-4f51-a4f5-2ed915b93039",
   "metadata": {},
   "source": [
    "## Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f664d97f-b5b9-4c26-9ff1-9c0a73a15827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from IPython.display import clear_output\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f06dd56-8f0c-4aac-bd89-c5a510348db9",
   "metadata": {},
   "source": [
    "## Collecting input\n",
    "- Provide the tokens full filename\n",
    "- Provide the stoplist full filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9a82850-dfa7-4851-adae-f891c01bd2b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "end = False\n",
    "while end == False:\n",
    "    filename = str(input('Enter the tokens full filename: '))\n",
    "    if filename != '':\n",
    "        try:\n",
    "            with open(filename, 'r', encoding = 'utf8') as tokens:\n",
    "                print('The file exists.')\n",
    "            input_file = filename\n",
    "            output_file = input_file + '.prepared.txt'\n",
    "            end = True\n",
    "        except FileNotFoundError:\n",
    "            print('No such file.')\n",
    "end = False\n",
    "while end == False:\n",
    "    filename = str(input('Enter the stoplist full filename: '))\n",
    "    if filename != '':\n",
    "        try:\n",
    "            with open(filename, 'r', encoding = 'utf8') as stoplist:\n",
    "                print('The file exists.')\n",
    "            stoplist_file = filename\n",
    "            output_file = input_file + '.prepared.txt'\n",
    "            end = True\n",
    "            clear_output()\n",
    "        except FileNotFoundError:\n",
    "            print('No such file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3485cde9-606a-444a-8599-81056e000410",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e918f33-34a7-428c-8674-166ae55e4012",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['wsl', 'tr', '-s', \"' '\", '>', 'group_4_tokens.txt.prepared.txt'], returncode=0, stdout=b'', stderr=b'')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "substitutions = subprocess.run(['wsl', 'sed',  '-f',  stoplist_file, input_file], stdout = subprocess.PIPE)\n",
    "subprocess.run(['wsl', 'tr', '-s', \"' '\", '>', output_file], input = substitutions.stdout, capture_output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4aea23-c00f-4760-b7e7-fd5fefb0f33c",
   "metadata": {},
   "source": [
    "## Processing the Topical Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6256dd59-23ad-4093-b6c9-bce6214b3300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your text data from the file into a list\n",
    "text_data = []\n",
    "\n",
    "input_file_path = output_file  # Update with the actual input file path\n",
    "\n",
    "with open(input_file_path, 'r', encoding = 'utf-8') as input_file1:\n",
    "    for line in input_file1:\n",
    "        fields = line.strip().split('|')\n",
    "        \n",
    "        if len(fields) >= 4:\n",
    "            content_to_analyze = fields[3]  # Assuming field 7 is at index 6\n",
    "            text_data.append((fields[0], content_to_analyze))  # Store ID and content\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess(text):\n",
    "    return simple_preprocess(text, deacc = True)\n",
    "\n",
    "processed_text_data = [preprocess(text) for _, text in text_data]\n",
    "\n",
    "# Create a dictionary representation of the documents\n",
    "dictionary = corpora.Dictionary(processed_text_data)\n",
    "\n",
    "# Create a corpus: a list of bags of words\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_text_data]\n",
    "\n",
    "# Train the LDA model\n",
    "num_topics = 2  # You can adjust the number of topics\n",
    "lda_model = LdaModel(corpus, num_topics = num_topics, id2word = dictionary, passes = 15)\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "\n",
    "# Score each text and gather results\n",
    "for (id_value, content), text in zip(text_data, processed_text_data):\n",
    "    doc_bow = dictionary.doc2bow(text)\n",
    "    doc_topics = lda_model[doc_bow]\n",
    "    \n",
    "    # Calculate scores for each topic and store in a dictionary\n",
    "    topic_scores = {topic: score for topic, score in doc_topics}\n",
    "    \n",
    "    # Find the topic with the highest score\n",
    "    top_topic = max(topic_scores, key=topic_scores.get)\n",
    "    top_score = topic_scores[top_topic]\n",
    "    \n",
    "    # Prepare the result dictionary\n",
    "    result = {\n",
    "        'id': id_value,\n",
    "        'topic': top_topic,\n",
    "        'score': top_score,\n",
    "        'c': content\n",
    "    }\n",
    "    \n",
    "    results.append(result)\n",
    "\n",
    "# Save results to a file\n",
    "output_file_path = output_file + '.topic_scores_output.txt'  # Update with desired output file path\n",
    "\n",
    "with open(output_file_path, 'w', encoding = 'utf-8') as output_file1:\n",
    "    for result in results:\n",
    "        output_file1.write(f\"{result['id']}|topic:{result['topic']}|score:{result['score']}|c:{result['c']}\\n\")\n",
    "\n",
    "# Get the list of topics with top words\n",
    "topics = lda_model.print_topics(num_words = 50)\n",
    "\n",
    "# Save topics to a file\n",
    "output_file_path = output_file + '.topic_words_output.txt'  # Update with desired output file path\n",
    "\n",
    "with open(output_file_path, 'w', encoding = 'utf-8') as output_file2:\n",
    "    for topic_num, words in topics:\n",
    "        output_file2.write(f\"Topic {topic_num + 1} Words: {words}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0337bb5-91ee-4abc-9aff-75933d334b94",
   "metadata": {},
   "source": [
    "## Processing the Score Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87b4c761-1e29-4102-b8cd-aaf219ed6f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring completed. Scored corpus saved in group_4_tokens.txt.prepared.txt.corpus_scored.txt.\n"
     ]
    }
   ],
   "source": [
    "# Read topic scores from the first file\n",
    "topic_scores = {}\n",
    "with open(output_file + '.topic_scores_output.txt', 'r', encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split('|')\n",
    "        if len(parts) >= 3:\n",
    "            id_part = parts[0]\n",
    "            score_part = parts[2]\n",
    "            topic_scores[id_part] = (score_part, parts[1])\n",
    "\n",
    "# Read and process the corpus file\n",
    "output_lines = []\n",
    "with open(input_file, 'r', encoding = 'utf-8') as f:  # Update with the actual input file path\n",
    "    for line in f:\n",
    "        parts = line.strip().split('|')\n",
    "        id_part = parts[0]\n",
    "        if id_part in topic_scores:\n",
    "            score_part, topic_part = topic_scores[id_part]\n",
    "            parts.insert(2, f\"{topic_part}\")\n",
    "            parts.insert(3, f\"{score_part}\")\n",
    "        output_line = '|'.join(parts)\n",
    "        output_lines.append(output_line)\n",
    "\n",
    "# Save the scored corpus to the output file\n",
    "output_filename = output_file + '.corpus_scored.txt'\n",
    "with open(output_filename, 'w', encoding='utf-8') as output_file3:\n",
    "    for line in output_lines:\n",
    "        output_file3.write(line + '\\n')\n",
    "\n",
    "print('Scoring completed. Scored corpus saved in ' + output_filename + '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43be553e-3065-4c0d-9f1d-a6e26b1e86e7",
   "metadata": {},
   "source": [
    "## Determining the topic words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "640f5d17-c72e-4c56-a70f-aa62db54e482",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output written to group_4_tokens.txt.prepared.txt.topic_words_formatted.txt\n"
     ]
    }
   ],
   "source": [
    "# Specify the input and output file paths\n",
    "input_file_path = output_file + '.topic_words_output.txt'  # Replace with the actual input file path\n",
    "output_file_path = output_file + '.topic_words_formatted.txt'  # Replace with the desired output file path\n",
    "\n",
    "# Initialize an empty result string\n",
    "result = \"\"\n",
    "\n",
    "# Read input from the specified file\n",
    "with open(input_file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Loop through each line and process it\n",
    "for line in lines:\n",
    "    # Extract the topic number and words\n",
    "    topic_parts = line.split(':')\n",
    "    topic_number = topic_parts[0].strip()\n",
    "    topic_words = topic_parts[1].strip()\n",
    "    \n",
    "    # Extract individual words and their weights\n",
    "    word_weight_pairs = topic_words.split(' + ')\n",
    "    \n",
    "    # Create a list to store formatted word-weight pairs\n",
    "    formatted_pairs = []\n",
    "    \n",
    "    for pair in word_weight_pairs:\n",
    "        parts = pair.split('*\"')\n",
    "        weight = parts[0].strip('0')  # Remove leading zeros\n",
    "        word = parts[1].strip().strip('\"')\n",
    "        \n",
    "        # Replace underscores with backslash-underscore\n",
    "        word = word.replace('_', '\\\\_')\n",
    "        \n",
    "        formatted_pair = f\"{word} ({weight})\"\n",
    "        formatted_pairs.append(formatted_pair)\n",
    "    \n",
    "    # Combine the formatted word-weight pairs into a single line\n",
    "    formatted_line = ', '.join(formatted_pairs)\n",
    "    \n",
    "    # Add the formatted line to the result\n",
    "    result += f\"{topic_number}: {formatted_line}\\n\"\n",
    "\n",
    "# Write the result to the specified output file\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    output_file.write(result)\n",
    "\n",
    "print(f\"Output written to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4234b6-8091-46b4-b3f1-7f5c9dc97902",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
